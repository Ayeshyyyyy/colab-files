{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"127qMbrdFJRB4IPhgv94CUz8ZaFikzdXp","timestamp":1757166918674}],"authorship_tag":"ABX9TyMq4EpT28JRxrkr0T2og3n+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JR9-JMP9G7Gm","executionInfo":{"status":"ok","timestamp":1757166893078,"user_tz":-300,"elapsed":31334,"user":{"displayName":"Aisha Anwaar","userId":"14975336948485702045"}},"outputId":"662c5693-29bd-4a90-ca9e-66b75cc9a554"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using device for generation: cpu\n"]},{"output_type":"stream","name":"stderr","text":["Device set to use cpu\n"]},{"output_type":"stream","name":"stdout","text":["\n","âœ… RAG Chatbot ready â€” type questions below. Type 'exit' to quit.\n","\n","You: exit\n","Goodbye ðŸ‘‹\n"]}],"source":["\n","!pip install -q --upgrade langchain langchain-community sentence-transformers faiss-cpu \"transformers[torch]\" accelerate torch\n","\n","# 2) Now the implementation\n","try:\n","    import os\n","    import torch\n","    from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n","    from sentence_transformers import SentenceTransformer\n","    from langchain.text_splitter import RecursiveCharacterTextSplitter\n","    from langchain.schema import Document\n","    from langchain.vectorstores import FAISS\n","    from langchain.embeddings import HuggingFaceEmbeddings\n","    from langchain.memory import ConversationBufferMemory\n","    from langchain.chains import ConversationalRetrievalChain\n","    from langchain.llms import HuggingFacePipeline\n","except Exception as e:\n","    raise RuntimeError(\n","        \"Import failed. Make sure the pip install step above ran successfully.\\n\"\n","        \"If you're in Colab, try restarting the runtime (Runtime -> Restart runtime) and re-run this cell.\\n\\n\"\n","        f\"Original error: {e}\"\n","    )\n","\n","# 3) Prepare a small example corpus (changed content)\n","corpus_text = \"\"\"Machine learning enables computers to learn patterns from data without explicit programming.\n","Vector databases like FAISS make it possible to search large collections of embeddings quickly.\n","Transformers are neural network architectures designed for handling sequential data.\n","Jupyter notebooks are widely used for interactive coding and data analysis.\n","Streamlit helps developers convert Python scripts into shareable web apps with minimal effort.\"\"\"\n","\n","# 4) Chunk into documents\n","splitter = RecursiveCharacterTextSplitter(chunk_size=400, chunk_overlap=50)\n","docs = splitter.create_documents([corpus_text])\n","\n","# 5) Build embeddings with sentence-transformers (via LangChain wrapper)\n","embed_model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n","embeddings = HuggingFaceEmbeddings(model_name=embed_model_name)\n","\n","# 6) Create FAISS index from documents\n","vectorstore = FAISS.from_documents(docs, embeddings)\n","\n","# 7) Prepare a small local LLM (flan-t5-small) for generation\n","hf_model_name = \"google/flan-t5-small\"\n","device = 0 if torch.cuda.is_available() else -1\n","\n","print(f\"Using device for generation: {'cuda' if torch.cuda.is_available() else 'cpu'}\")\n","\n","tokenizer = AutoTokenizer.from_pretrained(hf_model_name)\n","model = AutoModelForSeq2SeqLM.from_pretrained(hf_model_name)\n","if torch.cuda.is_available():\n","    model = model.to(\"cuda\")\n","\n","gen_pipeline = pipeline(\n","    \"text2text-generation\",\n","    model=model,\n","    tokenizer=tokenizer,\n","    device=device,\n","    framework=\"pt\",\n","    max_length=256,\n","    do_sample=False,\n",")\n","\n","llm = HuggingFacePipeline(pipeline=gen_pipeline)\n","\n","# 8) Create memory and the Conversational Retrieval Chain\n","memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n","\n","qa_chain = ConversationalRetrievalChain.from_llm(\n","    llm=llm,\n","    retriever=vectorstore.as_retriever(search_kwargs={\"k\": 4}),\n","    memory=memory,\n","    return_source_documents=True\n",")\n","\n","# 9) Colab-friendly chat loop\n","print(\"\\nâœ… RAG Chatbot ready â€” type questions below. Type 'exit' to quit.\\n\")\n","while True:\n","    q = input(\"You: \").strip()\n","    if not q:\n","        continue\n","    if q.lower() in (\"exit\", \"quit\"):\n","        print(\"Goodbye ðŸ‘‹\")\n","        break\n","\n","    result = qa_chain({\"question\": q})\n","    answer = result.get(\"answer\", \"\").strip()\n","    src_docs = result.get(\"source_documents\", [])\n","\n","    print(\"\\nBot:\", answer, \"\\n\")\n","    if src_docs:\n","        print(\"----- Retrieved Sources -----\")\n","        for i, d in enumerate(src_docs, 1):\n","            src = d.metadata.get(\"source\", f\"doc_{i}\")\n","            preview = d.page_content.replace(\"\\n\", \" \")[:400]\n","            print(f\"[{i}] source={src} | preview: {preview}\")\n","        print(\"-----------------------------\\n\")\n"]}]}